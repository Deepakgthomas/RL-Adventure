{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            print(\"q_value = \", q_value)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "            \n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20746a9a2b0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_value =  tensor([[-2.6989,  0.5761,  0.8147,  3.9889,  0.3014, -1.6791]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6028,  0.6299,  0.7094,  4.0689,  0.1468, -1.4263]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6102,  0.4796,  0.9059,  4.1313,  0.3484, -1.6367]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6925,  0.6709,  0.7897,  4.1457,  0.3313, -1.7349]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5977,  0.6182,  0.8945,  4.1421,  0.3236, -1.6292]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5978,  0.6724,  0.8796,  4.2104,  0.3624, -1.6260]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5438,  0.6850,  0.9182,  4.1499,  0.3446, -1.6320]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5957,  0.6387,  0.8130,  4.2147,  0.3515, -1.7354]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6180,  0.6379,  0.8463,  4.1979,  0.3442, -1.6199]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5822,  0.5452,  0.8892,  4.1601,  0.3333, -1.5591]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6030,  0.6540,  0.8035,  4.1762,  0.3066, -1.5540]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.4321,  0.8300,  1.0135,  4.1873,  0.4550, -1.5850]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6072,  0.6385,  0.9422,  4.2105,  0.3400, -1.6597]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5557,  0.6808,  0.9009,  4.1699,  0.3429, -1.6192]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5057,  0.5842,  0.7873,  4.1672,  0.3896, -1.7548]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6818,  0.6385,  0.8218,  4.1890,  0.2763, -1.5746]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6590,  0.6274,  0.8496,  4.1864,  0.4002, -1.6505]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6280,  0.6601,  0.8557,  4.2138,  0.3722, -1.5419]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5975,  0.6887,  0.8783,  4.1167,  0.3363, -1.5858]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6010,  0.5350,  0.8954,  4.0958,  0.3413, -1.6515]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6330,  0.6284,  1.0020,  4.1942,  0.2693, -1.6612]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6471,  0.7422,  0.8960,  4.1326,  0.2878, -1.5783]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.7066,  0.5844,  0.8822,  4.1128,  0.2273, -1.7132]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6557,  0.5686,  0.9459,  4.1532,  0.3386, -1.5884]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6587,  0.5604,  0.8993,  4.0602,  0.3441, -1.6719]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5931,  0.4666,  0.9091,  4.1959,  0.3971, -1.7099]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6165,  0.6321,  1.0858,  4.0998,  0.5544, -1.5741]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6337,  0.7018,  0.7643,  4.0966,  0.2608, -1.5734]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5613,  0.7374,  0.8879,  4.1788,  0.3194, -1.6732]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5531,  0.6863,  0.8053,  4.1136,  0.2735, -1.5549]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5555,  0.6493,  0.8497,  4.0809,  0.2625, -1.6855]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6155,  0.6752,  0.7969,  4.1554,  0.2442, -1.6119]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6280,  0.7868,  0.7649,  4.0520,  0.2599, -1.7376]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.3993,  0.6719,  0.8333,  3.9122,  0.2041, -1.7074]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5972,  0.4564,  0.9160,  4.3083,  0.3559, -1.5686]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6640,  0.7270,  0.7908,  4.2131,  0.2741, -1.5255]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6105,  0.7612,  0.8403,  4.0417,  0.3037, -1.6417]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6369,  0.7252,  0.7236,  4.1647,  0.3443, -1.5566]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6256,  0.6163,  0.7743,  4.1852,  0.4814, -1.6941]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6447,  0.4514,  0.9105,  4.0783,  0.2249, -1.6958]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5760,  0.4859,  0.8770,  4.1149,  0.3230, -1.6461]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6648,  0.5818,  0.9941,  4.1256,  0.4315, -1.5416]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5820,  0.6429,  0.9077,  4.2014,  0.3360, -1.5835]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6263,  0.4804,  1.0576,  4.1685,  0.3435, -1.7351]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.4725,  0.6359,  0.8363,  4.1286,  0.3414, -1.7124]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6224,  0.6973,  0.9408,  4.1790,  0.3346, -1.6113]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5951,  0.6312,  0.8524,  4.2115,  0.4044, -1.7268]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6684,  0.5829,  0.9896,  4.1458,  0.3464, -1.6634]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5302,  0.6448,  0.8230,  4.0405,  0.3712, -1.6709]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6149,  0.7199,  0.9937,  4.1079,  0.3664, -1.6151]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6138,  0.6146,  0.7835,  4.1625,  0.3678, -1.7763]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6796,  0.5235,  0.8343,  4.0831,  0.2989, -1.7189]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5505,  0.7411,  0.8425,  4.2177,  0.3403, -1.4935]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6344,  0.6456,  0.8680,  4.1663,  0.3043, -1.5719]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5485,  0.5942,  1.1154,  4.1657,  0.3174, -1.7603]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6986,  0.5118,  0.8482,  4.1336,  0.3997, -1.4914]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6590,  0.5776,  0.6465,  4.1787,  0.3879, -1.6061]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5455,  0.7242,  0.7380,  4.0778,  0.3089, -1.5716]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5803,  0.5317,  0.7795,  4.0672,  0.3317, -1.6698]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.7060,  0.4756,  0.7500,  4.2335,  0.4860, -1.6556]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6093,  0.5092,  1.0208,  4.1431,  0.3315, -1.7708]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.5721,  0.6514,  0.8394,  4.0678,  0.3440, -1.6752]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6053,  0.6520,  0.8970,  4.1417,  0.3276, -1.5930]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "q_value =  tensor([[-2.6110,  0.6581,  0.8939,  4.1671,  0.3550, -1.6277]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6317f43a3806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\RL-Adventure\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, ac)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\RL-Adventure\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\RL-Adventure\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\RL-Adventure\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, ac)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFireResetEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_frames = 1400000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
